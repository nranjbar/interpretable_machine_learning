# interpretable_machine_learning

Related paper: Using Decision Tree as Local Interpretable Model in Autoencoder-based LIME

All of the files used by humans for interpretability evaluation are in the zip file (interpretability_test.rar).

Link of codes and APIs:

https://www.kaggle.com/rspadim/simple-denoise-autoencoder-with-keras

https://huggingface.co/transformers/model_doc/xlnet.html##xlnettokenizer

https://github.com/shanayghag/Sentiment-classification-using-XLNet/blob/master/Sentiment_Analysis_Series_part_1.ipynb

https://huggingface.co/transformers/model_doc/xlnet.html

https://nlpaug.readthedocs.io/en/latest/##nlpaug

https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html

https://www.nltk.org/_modules/nltk/corpus/reader/sentiwordnet.html

https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html

https://scikit-learn.org/stable/modules/tree.html
