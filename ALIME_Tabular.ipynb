{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ALIME_Tabular.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ghwsr_dDQZ8z",
        "A0iAWZ8eQg9j",
        "cEDXJeUYQvEY",
        "r7n8T0p6RXbj",
        "VqfiMgI8RonK",
        "Bok9uYLyRwsk"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7c2Vl-bTP-r"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np # linear algebra\n",
        "import keras \n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Input, Concatenate, Dropout\n",
        "import tensorflow as tf\n",
        "import sklearn\n",
        "from sklearn import *\n",
        "import heapq\n",
        "import scipy as sp\n",
        "from sklearn.utils import check_random_state\n",
        "from abc import ABCMeta, abstractmethod\n",
        "from functools import partial\n",
        "import collections"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghwsr_dDQZ8z"
      },
      "source": [
        "# Read Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kP3gioKruy0"
      },
      "source": [
        "def read_data(dataset_name):\n",
        "  if dataset_name is 'breast_cancer':\n",
        "    data = datasets.load_breast_cancer()\n",
        "    feature_names = data .feature_names\n",
        "    target_names = data .target_names\n",
        "    (train,test,labels_train,labels_test) = model_selection.train_test_split(data.data, data.target, train_size=0.80)\n",
        "    return data,data.target,feature_names,target_names,train,labels_train,test,labels_test\n",
        "  elif dataset_name is 'hepatisis':\n",
        "    hepatisis = pd.read_csv('hepatisis.csv')\n",
        "    hepatisis.drop(['protime'],axis=1, inplace=True)\n",
        "    categorical=pd.DataFrame(hepatisis.select_dtypes(exclude='number')).columns\n",
        "    number=pd.DataFrame(hepatisis.select_dtypes(include='number')).columns\n",
        "    hepatisis.dropna(subset=categorical,inplace=True)\n",
        "    imp=sklearn.impute.SimpleImputer(strategy='mean')\n",
        "    numbers_cols=pd.DataFrame(hepatisis.loc[:,number], columns=number)\n",
        "    imp.fit(numbers_cols)\n",
        "    numbers_cols=imp.transform(numbers_cols)\n",
        "    hepatisis.drop(number,axis=1,inplace=True)\n",
        "    hepatisis.reset_index(drop=True,inplace=True)\n",
        "    numbers_data=pd.DataFrame(numbers_cols,columns=number)\n",
        "    hepatisis_main=pd.concat([hepatisis,numbers_data], axis=1)\n",
        "    data=hepatisis_main.drop(['class'],axis=1)\n",
        "    targets=hepatisis_main['class']\n",
        "    le= sklearn.preprocessing.LabelEncoder()\n",
        "    le.fit(targets)\n",
        "    labels = le.transform(targets)\n",
        "    class_names = le.classes_\n",
        "    categorical_features=range(13)                   \n",
        "    categorical_names = {}\n",
        "    for feature in categorical_features:\n",
        "        le = sklearn.preprocessing.LabelEncoder()\n",
        "        le.fit(data[categorical[feature]])\n",
        "        data[categorical[feature]] = le.transform(data[categorical[feature]])\n",
        "        categorical_names[feature] = le.classes_\n",
        "        \n",
        "    data = data.astype(float)\n",
        "    train, test, labels_train, labels_test = sklearn.model_selection.train_test_split(data, labels, train_size=0.80)\n",
        "    return data,labels,class_names,categorical_features,categorical_names,categorical,train,labels_train,test,labels_test\n",
        "  elif dataset_name is 'liver':\n",
        "      liver= pd.read_csv('IndianLiverPatientDataset(ILPD).csv')\n",
        "      categorical=pd.DataFrame(liver.select_dtypes(exclude='number')).columns\n",
        "      number=pd.DataFrame(liver.select_dtypes(include='number')).columns\n",
        "      liver.dropna(subset=categorical,inplace=True)\n",
        "      imp=sklearn.impute.SimpleImputer(strategy='mean')\n",
        "      numbers_cols=pd.DataFrame(liver.loc[:,number], columns=number)\n",
        "      imp.fit(numbers_cols)\n",
        "      numbers_cols=imp.transform(numbers_cols)\n",
        "      liver.drop(number,axis=1,inplace=True)\n",
        "      liver.reset_index(drop=True,inplace=True)\n",
        "      numbers_data=pd.DataFrame(numbers_cols,columns=number)\n",
        "      liver_main=pd.concat([liver,numbers_data], axis=1)\n",
        "      data=liver_main.drop(['label'],axis=1)\n",
        "      targets=liver_main['label']\n",
        "      le= sklearn.preprocessing.LabelEncoder()\n",
        "      le.fit(targets)\n",
        "      labels = le.transform(targets)\n",
        "      class_names = le.classes_\n",
        "      categorical_features=range(1)                   \n",
        "      categorical_names = {}\n",
        "      for feature in categorical_features:\n",
        "          le = sklearn.preprocessing.LabelEncoder()\n",
        "          le.fit(data[categorical[feature]])\n",
        "          data[categorical[feature]] = le.transform(data[categorical[feature]])\n",
        "          categorical_names[feature] = le.classes_     \n",
        "      data = data.astype(float)\n",
        "      train, test, labels_train, labels_test = sklearn.model_selection.train_test_split(data, labels, train_size=0.80)\n",
        "      return data,labels,class_names,categorical_features,categorical_names,categorical,train,labels_train,test,labels_test\n",
        "  elif dataset_name is 'adults':\n",
        "      adults = pd.read_csv('adults.csv')\n",
        "      categorical_num=range(6,105)\n",
        "      categorical=adults.iloc[:, categorical_num].columns\n",
        "      data=adults.drop(['class'],axis=1)\n",
        "      targets=adults['class']\n",
        "      le= sklearn.preprocessing.LabelEncoder()\n",
        "      le.fit(targets)\n",
        "      labels = le.transform(targets)\n",
        "      class_names = le.classes_\n",
        "      data = data.astype(float)\n",
        "      train, test, labels_train, labels_test = sklearn.model_selection.train_test_split(data, labels, train_size=0.80)\n",
        "      return data,labels,class_names,categorical,categorical_num,train,labels_train,test,labels_test\n",
        "  else:\n",
        "      print('dataset_name is not correct!')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7YcJ_Tiw1HM"
      },
      "source": [
        "def normalize_data(input_data, has_column):\n",
        "  if has_column:\n",
        "     numbers_data_scaled=pd.DataFrame(preprocessing.scale(input_data),columns=input_data.columns)\n",
        "  else:\n",
        "     numbers_data_scaled=pd.DataFrame(preprocessing.scale(input_data))\n",
        "  return numbers_data_scaled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "715pBWjbxLjd"
      },
      "source": [
        "data,labels,feature_names,target_names,train,labels_train,test,labels_test=read_data('breast_cancer')\n",
        "normalized_train=normalize_data(train,False)\n",
        "normalized_test=normalize_data(test,False)\n",
        "normalized_data=normalize_data(data.data,False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZL8S5gGgyJSL"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bffv1vFhxf-N"
      },
      "source": [
        "data,labels,class_names,categorical_features,categorical_names,categorical,train,labels_train,test,labels_test=read_data('hepatisis')\n",
        "normalized_train=normalize_data(train,True)\n",
        "normalized_test=normalize_data(test,True)\n",
        "normalized_data=normalize_data(data,True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Af3R2Ziyxv8H"
      },
      "source": [
        "data,labels,class_names,categorical_features,categorical_names,categorical,train,labels_train,test,labels_test=read_data('liver')\n",
        "normalized_train=normalize_data(train,True)\n",
        "normalized_test=normalize_data(test,True)\n",
        "normalized_data=normalize_data(data,True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzkyfiEWx2hN"
      },
      "source": [
        "data,labels,class_names,categorical,categorical_num,train,labels_train,test,labels_test=read_data('adults')\n",
        "normalized_train=normalize_data(train,True)\n",
        "normalized_test=normalize_data(test,True)\n",
        "normalized_data=normalize_data(data,True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0iAWZ8eQg9j"
      },
      "source": [
        "# Train denoising Auto-encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MtV3js4ZST0"
      },
      "source": [
        "import threading\n",
        "class ReadWriteLock:\n",
        "    def __init__(self):\n",
        "        self._read_ready = threading.Condition(threading.Lock())\n",
        "        self._readers = 0\n",
        "    def acquire_read(self):\n",
        "        self._read_ready.acquire()\n",
        "        try:\n",
        "            self._readers += 1\n",
        "        finally:\n",
        "            self._read_ready.release()\n",
        "    def release_read(self):\n",
        "        self._read_ready.acquire()\n",
        "        try:\n",
        "            self._readers -= 1\n",
        "            if not self._readers:\n",
        "                self._read_ready.notifyAll()\n",
        "        finally:\n",
        "            self._read_ready.release()\n",
        "    def acquire_write(self):\n",
        "        self._read_ready.acquire()\n",
        "        while self._readers > 0:\n",
        "            self._read_ready.wait()\n",
        "    def release_write(self):\n",
        "        self._read_ready.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSKlruN2ZUrb"
      },
      "source": [
        "from math import ceil\n",
        "class DAESequence(Sequence):\n",
        "    def __init__(self, df, batch_size=128, random_cols=.15, random_rows=1, use_cache=False, use_lock=False, verbose=True):\n",
        "        self.df = df.values.copy()     # ndarray baby\n",
        "        self.batch_size = int(batch_size)\n",
        "        self.len_data = df.shape[0]\n",
        "        self.len_input_columns = df.shape[1]\n",
        "        if(random_cols <= 0):\n",
        "            self.random_cols = 0\n",
        "        elif(random_cols >= 1):\n",
        "            self.random_cols = self.len_input_columns\n",
        "        else:\n",
        "            self.random_cols = int(random_cols*self.len_input_columns)\n",
        "        if(self.random_cols > self.len_input_columns):\n",
        "            self.random_cols = self.len_input_columns\n",
        "        self.random_rows = random_rows\n",
        "        self.cache = None\n",
        "        self.use_cache = use_cache\n",
        "        self.use_lock = use_lock\n",
        "        self.verbose = verbose\n",
        "        \n",
        "        self.lock = ReadWriteLock()\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if(not self.use_cache):\n",
        "            return\n",
        "        if(self.use_lock):\n",
        "            self.lock.acquire_write()\n",
        "        if(self.verbose):\n",
        "            print(\"Doing Cache\")\n",
        "        self.cache = {}\n",
        "        for i in range(0, self.__len__()):\n",
        "            self.cache[i] = self.__getitem__(i, True)\n",
        "        if(self.use_lock):\n",
        "            self.lock.release_write()\n",
        "        gc.collect()\n",
        "        if(self.verbose):\n",
        "            print(\"Done\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(ceil(self.len_data / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx, doing_cache=False):\n",
        "        if(not doing_cache and self.cache is not None and not (self.random_cols <=0 or self.random_rows<=0)):\n",
        "            if(idx in self.cache.keys()):\n",
        "                if(self.use_lock):\n",
        "                    self.lock.acquire_read()\n",
        "                ret0, ret1 = self.cache[idx][0], self.cache[idx][1]\n",
        "                if(self.use_lock):\n",
        "                    self.lock.release_read()\n",
        "                if (not doing_cache and self.verbose):\n",
        "                    print('DAESequence Cache ', idx)\n",
        "                return ret0, ret1\n",
        "        idx_end = min(idx + self.batch_size, self.len_data)\n",
        "        cur_len = idx_end - idx\n",
        "        rows_to_sample = int(self.random_rows * cur_len)\n",
        "        input_x = self.df[idx: idx_end]\n",
        "        if (self.random_cols <= 0 or self.random_rows <= 0 or rows_to_sample<=0):\n",
        "            return input_x, input_x # not dae\n",
        "        # here start the magic\n",
        "        random_rows = np.random.randint(low=0, high=self.len_data-rows_to_sample, size=rows_to_sample)\n",
        "        random_rows[random_rows>idx] += cur_len # just to don't select twice the current rows\n",
        "        cols_to_shuffle = np.random.randint(low=0, high=self.len_input_columns, size=self.random_cols)\n",
        "        noise_x = input_x.copy()\n",
        "        noise_x[0:rows_to_sample, cols_to_shuffle] = self.df[random_rows[:,None], cols_to_shuffle]\n",
        "        if(not doing_cache and self.verbose):\n",
        "            print('DAESequence ', idx)\n",
        "        return noise_x, input_x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njKF21J9aTXt",
        "outputId": "9ff38dbd-f609-407c-d13e-f96d381e0dbe"
      },
      "source": [
        "print(\"Create Model\")\n",
        "dae_data=normalized_train\n",
        "len_input_columns, len_data = dae_data.shape[1], dae_data.shape[0]\n",
        "NUM_GPUS=1\n",
        "#kernel_initializer='Orthogonal'  # this one give non NaN more often than others \n",
        "\n",
        "# from https://kaggle2.blob.core.windows.net/forum-message-attachments/250927/8325/nn.cfg.log\n",
        "#L0: 221(in)-1500 'r'ReLU  lRate:0.003 lRateDecay:0.995 regL2:0 regL1:0 dropout:0  w:222x1500  out(x3):1501x128 (0.00210051 GB) init..(uni:1 sp:1)[min|max|mean|std:-0.0672672|0.0672671|-4.74564e-05|0.0388202]\n",
        "#L1: 1500(in)-1500 'r'ReLU  lRate:0.003 lRateDecay:0.995 regL2:0 regL1:0 dropout:0  w:1501x1500  out(x3):1501x128 (0.00977451 GB) init..(uni:1 sp:1)[min|max|mean|std:-0.0258199|0.0258199|8.51905e-06|0.0148989]\n",
        "#L2: 1500(in)-1500 'r'ReLU  lRate:0.003 lRateDecay:0.995 regL2:0 regL1:0 dropout:0  w:1501x1500  out(x3):1501x128 (0.00977451 GB) init..(uni:1 sp:1)[min|max|mean|std:-0.0258199|0.0258199|8.51905e-06|0.0148989]\n",
        "#L3: 1500(in)-221 'l'linear  lRate:0.003 lRateDecay:0.995 regL2:0 regL1:0 dropout:0  w:1501x221  out(x3):222x128 (0.00144055 GB) init..(uni:1 sp:1)[min|max|mean|std:-0.0258199|0.0258198|-1.80977e-05|0.0149005]\n",
        "\n",
        "# is uni:1 = uniform? what about sp:1 ?\n",
        "# std ~= sqrt(2 / (input+output))   (first layer)\n",
        "# std ~= sqrt(1 / (input+output))   (others layer)\n",
        "kernel_initializer_0=tf.keras.initializers.RandomNormal(mean=-4.74564e-05, stddev=0.0388202, seed=None)   # sqrt(2/(221+1500)) = 0.0341 vs 0,0388\n",
        "kernel_initializer_1=tf.keras.initializers.RandomNormal(mean=8.51905e-06, stddev=0.0148989, seed=None)    # sqrt(1/(1500+1500)) = 0.018 vs 0,014\n",
        "kernel_initializer_2=tf.keras.initializers.RandomNormal(mean=8.51905e-06, stddev=0.0148989, seed=None)    # sqrt(1/(1500+1500)) = 0.018 vs 0,014\n",
        "kernel_initializer_3=tf.keras.initializers.RandomNormal(mean=-1.80977e-05, stddev=0.0149005, seed=None)   # sqrt(1/(1500+221)) = 0.024 vs 0,014\n",
        "\n",
        "print(\"Input len=\", len_input_columns, len_data)\n",
        "model_dae = Sequential()\n",
        "model_dae.add(Dense(units=len_input_columns*10, activation='relu', dtype='float32', name='Hidden1', input_shape=(len_input_columns,), kernel_initializer=kernel_initializer_0))\n",
        "model_dae.add(Dense(units=len_input_columns*10, activation='relu', dtype='float32', name='Hidden2', kernel_initializer=kernel_initializer_1))\n",
        "model_dae.add(Dense(units=len_input_columns*10, activation='relu', dtype='float32', name='Hidden3', kernel_initializer=kernel_initializer_2))\n",
        "model_dae.add(Dense(units=len_input_columns, activation='linear', dtype='float32', name='Output', kernel_initializer=kernel_initializer_3))\n",
        "model_opt = keras.optimizers.SGD(lr=0.003, decay=1-0.995, momentum=0, nesterov=False) # decay -> Oscar Takeshita comment\n",
        "\n",
        "try:\n",
        "    print('Loading model from file')\n",
        "    model_dae = keras.models.load_model('DAE.keras.model.h5')\n",
        "except Exception as e:\n",
        "    print(\"Can't load previous fitting parameters and model\", repr(e))\n",
        "if(NUM_GPUS>1):\n",
        "    try:\n",
        "        multi_gpu_model = keras.utils.multi_gpu_model(model_dae, gpus=NUM_GPUS)\n",
        "        multi_gpu_model.compile(loss='mean_squared_error', optimizer=model_opt)\n",
        "        print(\"MULTI GPU MODEL\")\n",
        "        print(multi_gpu_model.summary())\n",
        "    except Exception as e:\n",
        "        print(\"Can't run multi gpu, error=\", repr(e))\n",
        "        model_dae.compile(loss='mean_squared_error', optimizer=model_opt)\n",
        "        NUM_GPUS=0\n",
        "else:\n",
        "    model_dae.compile(loss='mean_squared_error', optimizer=model_opt)\n",
        "\n",
        "print(\"BASE MODEL\")\n",
        "print(model_dae.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Create Model\n",
            "Input len= 105 39073\n",
            "Loading model from file\n",
            "Can't load previous fitting parameters and model OSError('SavedModel file does not exist at: DAE.keras.model.h5/{saved_model.pbtxt|saved_model.pb}')\n",
            "BASE MODEL\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "Hidden1 (Dense)              (None, 1050)              111300    \n",
            "_________________________________________________________________\n",
            "Hidden2 (Dense)              (None, 1050)              1103550   \n",
            "_________________________________________________________________\n",
            "Hidden3 (Dense)              (None, 1050)              1103550   \n",
            "_________________________________________________________________\n",
            "Output (Dense)               (None, 105)               110355    \n",
            "=================================================================\n",
            "Total params: 2,428,755\n",
            "Trainable params: 2,428,755\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKILApoudMqV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "80807b03-1460-4e37-85d9-4ccbb690f6e2"
      },
      "source": [
        "from math import ceil\n",
        "batch_size =128\n",
        "multi_process_workers = 2\n",
        "if (NUM_GPUS > 1):\n",
        "    multi_gpu_model.fit_generator(\n",
        "        DAESequence(dae_data, batch_size=batch_size*NUM_GPUS, verbose=False),\n",
        "        steps_per_epoch=int(ceil(dae_data.shape[0]/(batch_size*NUM_GPUS))),\n",
        "        workers=multi_process_workers, use_multiprocessing=True if multi_process_workers>1 else False,\n",
        "        epochs=500,\n",
        "        verbose=1,\n",
        "        callbacks=[\n",
        "            # keras.callbacks.LambdaCallback(on_epoch_end=lambda x,y: model_dae.save('DAE.keras.model.h5')) # save weights \n",
        "        ])\n",
        "else: # single CPU/GPU\n",
        "    model_dae.fit_generator(\n",
        "        DAESequence(dae_data, batch_size=batch_size, verbose=False),\n",
        "        steps_per_epoch=int(ceil(dae_data.shape[0]/batch_size)),\n",
        "        epochs=500,\n",
        "        workers=multi_process_workers, use_multiprocessing=True if multi_process_workers>1 else False,\n",
        "        verbose=1)\n",
        "    \n",
        "model_dae.save('DAE.keras.model.h5') # save weights\n",
        "plt.hist(model_dae.get_weights(), bins = 100)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "306/306 [==============================] - 3s 4ms/step - loss: 1.1721\n",
            "Epoch 2/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1709\n",
            "Epoch 3/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1702\n",
            "Epoch 4/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1696\n",
            "Epoch 5/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1692\n",
            "Epoch 6/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1689\n",
            "Epoch 7/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1686\n",
            "Epoch 8/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1684\n",
            "Epoch 9/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1682\n",
            "Epoch 10/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1680\n",
            "Epoch 11/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1678\n",
            "Epoch 12/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1676\n",
            "Epoch 13/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1675\n",
            "Epoch 14/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1674\n",
            "Epoch 15/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1672\n",
            "Epoch 16/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1671\n",
            "Epoch 17/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1670\n",
            "Epoch 18/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1670\n",
            "Epoch 19/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1669\n",
            "Epoch 20/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1667\n",
            "Epoch 21/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1666\n",
            "Epoch 22/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1666\n",
            "Epoch 23/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1664\n",
            "Epoch 24/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1665\n",
            "Epoch 25/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1663\n",
            "Epoch 26/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1663\n",
            "Epoch 27/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1662\n",
            "Epoch 28/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1662\n",
            "Epoch 29/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1660\n",
            "Epoch 30/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1660\n",
            "Epoch 31/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1659\n",
            "Epoch 32/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1659\n",
            "Epoch 33/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1658\n",
            "Epoch 34/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1658\n",
            "Epoch 35/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1657\n",
            "Epoch 36/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1656\n",
            "Epoch 37/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1656\n",
            "Epoch 38/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1656\n",
            "Epoch 39/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1655\n",
            "Epoch 40/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1654\n",
            "Epoch 41/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1654\n",
            "Epoch 42/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1653\n",
            "Epoch 43/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1654\n",
            "Epoch 44/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1652\n",
            "Epoch 45/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1653\n",
            "Epoch 46/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1653\n",
            "Epoch 47/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1653\n",
            "Epoch 48/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1651\n",
            "Epoch 49/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1650\n",
            "Epoch 50/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1650\n",
            "Epoch 51/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1650\n",
            "Epoch 52/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1651\n",
            "Epoch 53/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1650\n",
            "Epoch 54/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1649\n",
            "Epoch 55/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1649\n",
            "Epoch 56/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1648\n",
            "Epoch 57/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1649\n",
            "Epoch 58/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1648\n",
            "Epoch 59/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1648\n",
            "Epoch 60/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1648\n",
            "Epoch 61/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1646\n",
            "Epoch 62/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1647\n",
            "Epoch 63/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1648\n",
            "Epoch 64/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1648\n",
            "Epoch 65/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1645\n",
            "Epoch 66/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1645\n",
            "Epoch 67/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1646\n",
            "Epoch 68/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1644\n",
            "Epoch 69/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1645\n",
            "Epoch 70/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1644\n",
            "Epoch 71/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1644\n",
            "Epoch 72/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1644\n",
            "Epoch 73/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1644\n",
            "Epoch 74/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1643\n",
            "Epoch 75/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1643\n",
            "Epoch 76/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1642\n",
            "Epoch 77/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1643\n",
            "Epoch 78/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1642\n",
            "Epoch 79/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1643\n",
            "Epoch 80/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1642\n",
            "Epoch 81/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1642\n",
            "Epoch 82/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1641\n",
            "Epoch 83/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1642\n",
            "Epoch 84/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1641\n",
            "Epoch 85/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1641\n",
            "Epoch 86/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1641\n",
            "Epoch 87/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1640\n",
            "Epoch 88/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1640\n",
            "Epoch 89/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1641\n",
            "Epoch 90/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1640\n",
            "Epoch 91/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1640\n",
            "Epoch 92/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1639\n",
            "Epoch 93/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1639\n",
            "Epoch 94/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1639\n",
            "Epoch 95/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1639\n",
            "Epoch 96/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1639\n",
            "Epoch 97/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1639\n",
            "Epoch 98/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1638\n",
            "Epoch 99/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1639\n",
            "Epoch 100/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1638\n",
            "Epoch 101/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1637\n",
            "Epoch 102/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1638\n",
            "Epoch 103/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1637\n",
            "Epoch 104/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1637\n",
            "Epoch 105/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1637\n",
            "Epoch 106/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1637\n",
            "Epoch 107/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1635\n",
            "Epoch 108/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1637\n",
            "Epoch 109/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1637\n",
            "Epoch 110/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1637\n",
            "Epoch 111/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1636\n",
            "Epoch 112/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1636\n",
            "Epoch 113/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1636\n",
            "Epoch 114/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1635\n",
            "Epoch 115/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1635\n",
            "Epoch 116/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1635\n",
            "Epoch 117/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1635\n",
            "Epoch 118/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1634\n",
            "Epoch 119/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1636\n",
            "Epoch 120/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1635\n",
            "Epoch 121/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1636\n",
            "Epoch 122/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1635\n",
            "Epoch 123/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1634\n",
            "Epoch 124/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1635\n",
            "Epoch 125/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1634\n",
            "Epoch 126/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1634\n",
            "Epoch 127/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1633\n",
            "Epoch 128/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1634\n",
            "Epoch 129/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1634\n",
            "Epoch 130/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1634\n",
            "Epoch 131/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1633\n",
            "Epoch 132/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1632\n",
            "Epoch 133/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1633\n",
            "Epoch 134/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1632\n",
            "Epoch 135/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1634\n",
            "Epoch 136/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1632\n",
            "Epoch 137/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1633\n",
            "Epoch 138/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1632\n",
            "Epoch 139/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1631\n",
            "Epoch 140/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1633\n",
            "Epoch 141/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1633\n",
            "Epoch 142/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1631\n",
            "Epoch 143/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1632\n",
            "Epoch 144/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1632\n",
            "Epoch 145/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1631\n",
            "Epoch 146/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1631\n",
            "Epoch 147/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1632\n",
            "Epoch 148/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1630\n",
            "Epoch 149/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1631\n",
            "Epoch 150/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1630\n",
            "Epoch 151/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1630\n",
            "Epoch 152/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1630\n",
            "Epoch 153/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1631\n",
            "Epoch 154/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1631\n",
            "Epoch 155/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1631\n",
            "Epoch 156/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1630\n",
            "Epoch 157/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1630\n",
            "Epoch 158/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1630\n",
            "Epoch 159/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1630\n",
            "Epoch 160/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1630\n",
            "Epoch 161/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1631\n",
            "Epoch 162/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1630\n",
            "Epoch 163/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1630\n",
            "Epoch 164/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1629\n",
            "Epoch 165/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1629\n",
            "Epoch 166/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1628\n",
            "Epoch 167/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1629\n",
            "Epoch 168/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1629\n",
            "Epoch 169/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1628\n",
            "Epoch 170/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1629\n",
            "Epoch 171/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1629\n",
            "Epoch 172/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1628\n",
            "Epoch 173/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1629\n",
            "Epoch 174/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1627\n",
            "Epoch 175/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1627\n",
            "Epoch 176/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1627\n",
            "Epoch 177/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1627\n",
            "Epoch 178/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1627\n",
            "Epoch 179/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1628\n",
            "Epoch 180/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1629\n",
            "Epoch 181/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1628\n",
            "Epoch 182/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1628\n",
            "Epoch 183/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1626\n",
            "Epoch 184/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1628\n",
            "Epoch 185/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1627\n",
            "Epoch 186/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1627\n",
            "Epoch 187/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1628\n",
            "Epoch 188/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1627\n",
            "Epoch 189/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1627\n",
            "Epoch 190/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1626\n",
            "Epoch 191/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1626\n",
            "Epoch 192/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1625\n",
            "Epoch 193/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1626\n",
            "Epoch 194/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1626\n",
            "Epoch 195/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1627\n",
            "Epoch 196/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1626\n",
            "Epoch 197/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1627\n",
            "Epoch 198/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1626\n",
            "Epoch 199/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1626\n",
            "Epoch 200/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1625\n",
            "Epoch 201/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1625\n",
            "Epoch 202/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1626\n",
            "Epoch 203/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1625\n",
            "Epoch 204/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1625\n",
            "Epoch 205/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1625\n",
            "Epoch 206/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1626\n",
            "Epoch 207/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1626\n",
            "Epoch 208/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1625\n",
            "Epoch 209/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1625\n",
            "Epoch 210/500\n",
            "306/306 [==============================] - 2s 5ms/step - loss: 1.1626\n",
            "Epoch 211/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1625\n",
            "Epoch 212/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1623\n",
            "Epoch 213/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1625\n",
            "Epoch 214/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1624\n",
            "Epoch 215/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1625\n",
            "Epoch 216/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1624\n",
            "Epoch 217/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1625\n",
            "Epoch 218/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1624\n",
            "Epoch 219/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1625\n",
            "Epoch 220/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1625\n",
            "Epoch 221/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1623\n",
            "Epoch 222/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1624\n",
            "Epoch 223/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1623\n",
            "Epoch 224/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1624\n",
            "Epoch 225/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1622\n",
            "Epoch 226/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1624\n",
            "Epoch 227/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1624\n",
            "Epoch 228/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1624\n",
            "Epoch 229/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1623\n",
            "Epoch 230/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1623\n",
            "Epoch 231/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1624\n",
            "Epoch 232/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1624\n",
            "Epoch 233/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1622\n",
            "Epoch 234/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1623\n",
            "Epoch 235/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1622\n",
            "Epoch 236/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1622\n",
            "Epoch 237/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1624\n",
            "Epoch 238/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1622\n",
            "Epoch 239/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1621\n",
            "Epoch 240/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1622\n",
            "Epoch 241/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1622\n",
            "Epoch 242/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1622\n",
            "Epoch 243/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1622\n",
            "Epoch 244/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1623\n",
            "Epoch 245/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1621\n",
            "Epoch 246/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1621\n",
            "Epoch 247/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1622\n",
            "Epoch 248/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1622\n",
            "Epoch 249/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1622\n",
            "Epoch 250/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1622\n",
            "Epoch 251/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1622\n",
            "Epoch 252/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1623\n",
            "Epoch 253/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1621\n",
            "Epoch 254/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1622\n",
            "Epoch 255/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1621\n",
            "Epoch 256/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1622\n",
            "Epoch 257/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1622\n",
            "Epoch 258/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1622\n",
            "Epoch 259/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1621\n",
            "Epoch 260/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1622\n",
            "Epoch 261/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1621\n",
            "Epoch 262/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1621\n",
            "Epoch 263/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1621\n",
            "Epoch 264/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1620\n",
            "Epoch 265/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1621\n",
            "Epoch 266/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1621\n",
            "Epoch 267/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1621\n",
            "Epoch 268/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1621\n",
            "Epoch 269/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1621\n",
            "Epoch 270/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1621\n",
            "Epoch 271/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1619\n",
            "Epoch 272/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1621\n",
            "Epoch 273/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1620\n",
            "Epoch 274/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1621\n",
            "Epoch 275/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1619\n",
            "Epoch 276/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1621\n",
            "Epoch 277/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1621\n",
            "Epoch 278/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1621\n",
            "Epoch 279/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1619\n",
            "Epoch 280/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1620\n",
            "Epoch 281/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1619\n",
            "Epoch 282/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1619\n",
            "Epoch 283/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1620\n",
            "Epoch 284/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1619\n",
            "Epoch 285/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1620\n",
            "Epoch 286/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1619\n",
            "Epoch 287/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1619\n",
            "Epoch 288/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1618\n",
            "Epoch 289/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1620\n",
            "Epoch 290/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1619\n",
            "Epoch 291/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1619\n",
            "Epoch 292/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1617\n",
            "Epoch 293/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1619\n",
            "Epoch 294/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1619\n",
            "Epoch 295/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1619\n",
            "Epoch 296/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1618\n",
            "Epoch 297/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1620\n",
            "Epoch 298/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1619\n",
            "Epoch 299/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1618\n",
            "Epoch 300/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1617\n",
            "Epoch 301/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1619\n",
            "Epoch 302/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1618\n",
            "Epoch 303/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1618\n",
            "Epoch 304/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1618\n",
            "Epoch 305/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1619\n",
            "Epoch 306/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1618\n",
            "Epoch 307/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1619\n",
            "Epoch 308/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1619\n",
            "Epoch 309/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1618\n",
            "Epoch 310/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1619\n",
            "Epoch 311/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1619\n",
            "Epoch 312/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1617\n",
            "Epoch 313/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1618\n",
            "Epoch 314/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1619\n",
            "Epoch 315/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1618\n",
            "Epoch 316/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1619\n",
            "Epoch 317/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1617\n",
            "Epoch 318/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1618\n",
            "Epoch 319/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1618\n",
            "Epoch 320/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1617\n",
            "Epoch 321/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1618\n",
            "Epoch 322/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1619\n",
            "Epoch 323/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1617\n",
            "Epoch 324/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1617\n",
            "Epoch 325/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1618\n",
            "Epoch 326/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1619\n",
            "Epoch 327/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1618\n",
            "Epoch 328/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1618\n",
            "Epoch 329/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1617\n",
            "Epoch 330/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1616\n",
            "Epoch 331/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1616\n",
            "Epoch 332/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1618\n",
            "Epoch 333/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1618\n",
            "Epoch 334/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1616\n",
            "Epoch 335/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1616\n",
            "Epoch 336/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1617\n",
            "Epoch 337/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1617\n",
            "Epoch 338/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1616\n",
            "Epoch 339/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1616\n",
            "Epoch 340/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1617\n",
            "Epoch 341/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1616\n",
            "Epoch 342/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1617\n",
            "Epoch 343/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1618\n",
            "Epoch 344/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1617\n",
            "Epoch 345/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1616\n",
            "Epoch 346/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1617\n",
            "Epoch 347/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1616\n",
            "Epoch 348/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1616\n",
            "Epoch 349/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1616\n",
            "Epoch 350/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1616\n",
            "Epoch 351/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1617\n",
            "Epoch 352/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1617\n",
            "Epoch 353/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1616\n",
            "Epoch 354/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1617\n",
            "Epoch 355/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1615\n",
            "Epoch 356/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1616\n",
            "Epoch 357/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1618\n",
            "Epoch 358/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1616\n",
            "Epoch 359/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1616\n",
            "Epoch 360/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1617\n",
            "Epoch 361/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1616\n",
            "Epoch 362/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1616\n",
            "Epoch 363/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1616\n",
            "Epoch 364/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1614\n",
            "Epoch 365/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1616\n",
            "Epoch 366/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1616\n",
            "Epoch 367/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1617\n",
            "Epoch 368/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1615\n",
            "Epoch 369/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1616\n",
            "Epoch 370/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1616\n",
            "Epoch 371/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1616\n",
            "Epoch 372/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1615\n",
            "Epoch 373/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1615\n",
            "Epoch 374/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1615\n",
            "Epoch 375/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1617\n",
            "Epoch 376/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1615\n",
            "Epoch 377/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1615\n",
            "Epoch 378/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1614\n",
            "Epoch 379/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1614\n",
            "Epoch 380/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1615\n",
            "Epoch 381/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1615\n",
            "Epoch 382/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1615\n",
            "Epoch 383/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1615\n",
            "Epoch 384/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1613\n",
            "Epoch 385/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1615\n",
            "Epoch 386/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1614\n",
            "Epoch 387/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1614\n",
            "Epoch 388/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1614\n",
            "Epoch 389/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1615\n",
            "Epoch 390/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1614\n",
            "Epoch 391/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1613\n",
            "Epoch 392/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1614\n",
            "Epoch 393/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1615\n",
            "Epoch 394/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1613\n",
            "Epoch 395/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1615\n",
            "Epoch 396/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1615\n",
            "Epoch 397/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1614\n",
            "Epoch 398/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1615\n",
            "Epoch 399/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1615\n",
            "Epoch 400/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1616\n",
            "Epoch 401/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1614\n",
            "Epoch 402/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1614\n",
            "Epoch 403/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1615\n",
            "Epoch 404/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1615\n",
            "Epoch 405/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1614\n",
            "Epoch 406/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1613\n",
            "Epoch 407/500\n",
            "306/306 [==============================] - 1s 5ms/step - loss: 1.1614\n",
            "Epoch 408/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1613\n",
            "Epoch 409/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1614\n",
            "Epoch 410/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1614\n",
            "Epoch 411/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1613\n",
            "Epoch 412/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1615\n",
            "Epoch 413/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1614\n",
            "Epoch 414/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1614\n",
            "Epoch 415/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1613\n",
            "Epoch 416/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1613\n",
            "Epoch 417/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1614\n",
            "Epoch 418/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1614\n",
            "Epoch 419/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1612\n",
            "Epoch 420/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1615\n",
            "Epoch 421/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1614\n",
            "Epoch 422/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1615\n",
            "Epoch 423/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1615\n",
            "Epoch 424/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1615\n",
            "Epoch 425/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1613\n",
            "Epoch 426/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1614\n",
            "Epoch 427/500\n",
            "306/306 [==============================] - 1s 5ms/step - loss: 1.1613\n",
            "Epoch 428/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1613\n",
            "Epoch 429/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1614\n",
            "Epoch 430/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1612\n",
            "Epoch 431/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1612\n",
            "Epoch 432/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1614\n",
            "Epoch 433/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1615\n",
            "Epoch 434/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1614\n",
            "Epoch 435/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1613\n",
            "Epoch 436/500\n",
            "306/306 [==============================] - 1s 5ms/step - loss: 1.1613\n",
            "Epoch 437/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1614\n",
            "Epoch 438/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1614\n",
            "Epoch 439/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1613\n",
            "Epoch 440/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1614\n",
            "Epoch 441/500\n",
            "306/306 [==============================] - 1s 5ms/step - loss: 1.1611\n",
            "Epoch 442/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1614\n",
            "Epoch 443/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1613\n",
            "Epoch 444/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1612\n",
            "Epoch 445/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1612\n",
            "Epoch 446/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1615\n",
            "Epoch 447/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1612\n",
            "Epoch 448/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1614\n",
            "Epoch 449/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1612\n",
            "Epoch 450/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1614\n",
            "Epoch 451/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1612\n",
            "Epoch 452/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1612\n",
            "Epoch 453/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1612\n",
            "Epoch 454/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1613\n",
            "Epoch 455/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1612\n",
            "Epoch 456/500\n",
            "306/306 [==============================] - 2s 5ms/step - loss: 1.1612\n",
            "Epoch 457/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1612\n",
            "Epoch 458/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1612\n",
            "Epoch 459/500\n",
            "306/306 [==============================] - 1s 5ms/step - loss: 1.1612\n",
            "Epoch 460/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1611\n",
            "Epoch 461/500\n",
            "306/306 [==============================] - 1s 5ms/step - loss: 1.1613\n",
            "Epoch 462/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1611\n",
            "Epoch 463/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1613\n",
            "Epoch 464/500\n",
            "306/306 [==============================] - 1s 5ms/step - loss: 1.1610\n",
            "Epoch 465/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1611\n",
            "Epoch 466/500\n",
            "306/306 [==============================] - 2s 5ms/step - loss: 1.1612\n",
            "Epoch 467/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1611\n",
            "Epoch 468/500\n",
            "306/306 [==============================] - 2s 5ms/step - loss: 1.1612\n",
            "Epoch 469/500\n",
            "306/306 [==============================] - 1s 5ms/step - loss: 1.1613\n",
            "Epoch 470/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1612\n",
            "Epoch 471/500\n",
            "306/306 [==============================] - 1s 5ms/step - loss: 1.1611\n",
            "Epoch 472/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1611\n",
            "Epoch 473/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1613\n",
            "Epoch 474/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1613\n",
            "Epoch 475/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1610\n",
            "Epoch 476/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1612\n",
            "Epoch 477/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1611\n",
            "Epoch 478/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1613\n",
            "Epoch 479/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1612\n",
            "Epoch 480/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1611\n",
            "Epoch 481/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1612\n",
            "Epoch 482/500\n",
            "306/306 [==============================] - 1s 5ms/step - loss: 1.1610\n",
            "Epoch 483/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1612\n",
            "Epoch 484/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1612\n",
            "Epoch 485/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1611\n",
            "Epoch 486/500\n",
            "306/306 [==============================] - 2s 5ms/step - loss: 1.1611\n",
            "Epoch 487/500\n",
            "306/306 [==============================] - 1s 5ms/step - loss: 1.1611\n",
            "Epoch 488/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1612\n",
            "Epoch 489/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1612\n",
            "Epoch 490/500\n",
            "306/306 [==============================] - 1s 5ms/step - loss: 1.1611\n",
            "Epoch 491/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1612\n",
            "Epoch 492/500\n",
            "306/306 [==============================] - 2s 5ms/step - loss: 1.1612\n",
            "Epoch 493/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1612\n",
            "Epoch 494/500\n",
            "306/306 [==============================] - 2s 5ms/step - loss: 1.1610\n",
            "Epoch 495/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1611\n",
            "Epoch 496/500\n",
            "306/306 [==============================] - 1s 5ms/step - loss: 1.1611\n",
            "Epoch 497/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1610\n",
            "Epoch 498/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1610\n",
            "Epoch 499/500\n",
            "306/306 [==============================] - 1s 5ms/step - loss: 1.1611\n",
            "Epoch 500/500\n",
            "306/306 [==============================] - 1s 4ms/step - loss: 1.1610\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEDXJeUYQvEY"
      },
      "source": [
        "# Perturb data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdBhXqm7ZQEX"
      },
      "source": [
        "\"\"\"\n",
        "Discretizers classes, to be used in lime_tabular\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import sklearn.tree\n",
        "import scipy\n",
        "from sklearn.utils import check_random_state\n",
        "from abc import ABCMeta, abstractmethod\n",
        "\n",
        "\n",
        "class BaseDiscretizer():\n",
        "    \"\"\"\n",
        "    Abstract class - Build a class that inherits from this class to implement\n",
        "    a custom discretizer.\n",
        "    Method bins() is to be redefined in the child class, as it is the actual\n",
        "    custom part of the discretizer.\n",
        "    \"\"\"\n",
        "\n",
        "    __metaclass__ = ABCMeta  # abstract class\n",
        "\n",
        "    def __init__(self, data, categorical_features, feature_names, labels=None, random_state=None,\n",
        "                 data_stats=None):\n",
        "        \"\"\"Initializer\n",
        "        Args:\n",
        "            data: numpy 2d array\n",
        "            categorical_features: list of indices (ints) corresponding to the\n",
        "                categorical columns. These features will not be discretized.\n",
        "                Everything else will be considered continuous, and will be\n",
        "                discretized.\n",
        "            categorical_names: map from int to list of names, where\n",
        "                categorical_names[x][y] represents the name of the yth value of\n",
        "                column x.\n",
        "            feature_names: list of names (strings) corresponding to the columns\n",
        "                in the training data.\n",
        "            data_stats: must have 'means', 'stds', 'mins' and 'maxs', use this\n",
        "                if you don't want these values to be computed from data\n",
        "        \"\"\"\n",
        "        self.to_discretize = ([x for x in range(data.shape[1])\n",
        "                               if x not in categorical_features])\n",
        "        self.data_stats = data_stats\n",
        "        self.names = {}\n",
        "        self.lambdas = {}\n",
        "        self.means = {}\n",
        "        self.stds = {}\n",
        "        self.mins = {}\n",
        "        self.maxs = {}\n",
        "        self.random_state = check_random_state(random_state)\n",
        "\n",
        "        # To override when implementing a custom binning\n",
        "        bins = self.bins(data, labels)\n",
        "        bins = [np.unique(x) for x in bins]\n",
        "\n",
        "        # Read the stats from data_stats if exists\n",
        "        if data_stats:\n",
        "            self.means = self.data_stats.get(\"means\")\n",
        "            self.stds = self.data_stats.get(\"stds\")\n",
        "            self.mins = self.data_stats.get(\"mins\")\n",
        "            self.maxs = self.data_stats.get(\"maxs\")\n",
        "\n",
        "        for feature, qts in zip(self.to_discretize, bins):\n",
        "            n_bins = qts.shape[0]  # Actually number of borders (= #bins-1)\n",
        "            boundaries = np.min(data[:, feature]), np.max(data[:, feature])\n",
        "            name = feature_names[feature]\n",
        "\n",
        "            self.names[feature] = ['%s <= %.2f' % (name, qts[0])]\n",
        "            for i in range(n_bins - 1):\n",
        "                self.names[feature].append('%.2f < %s <= %.2f' %\n",
        "                                           (qts[i], name, qts[i + 1]))\n",
        "            self.names[feature].append('%s > %.2f' % (name, qts[n_bins - 1]))\n",
        "\n",
        "            self.lambdas[feature] = lambda x, qts=qts: np.searchsorted(qts, x)\n",
        "            discretized = self.lambdas[feature](data[:, feature])\n",
        "\n",
        "            # If data stats are provided no need to compute the below set of details\n",
        "            if data_stats:\n",
        "                continue\n",
        "\n",
        "            self.means[feature] = []\n",
        "            self.stds[feature] = []\n",
        "            for x in range(n_bins + 1):\n",
        "                selection = data[discretized == x, feature]\n",
        "                mean = 0 if len(selection) == 0 else np.mean(selection)\n",
        "                self.means[feature].append(mean)\n",
        "                std = 0 if len(selection) == 0 else np.std(selection)\n",
        "                std += 0.00000000001\n",
        "                self.stds[feature].append(std)\n",
        "            self.mins[feature] = [boundaries[0]] + qts.tolist()\n",
        "            self.maxs[feature] = qts.tolist() + [boundaries[1]]\n",
        "\n",
        "    @abstractmethod\n",
        "    def bins(self, data, labels):\n",
        "        \"\"\"\n",
        "        To be overridden\n",
        "        Returns for each feature to discretize the boundaries\n",
        "        that form each bin of the discretizer\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Must override bins() method\")\n",
        "\n",
        "    def discretize(self, data):\n",
        "        \"\"\"Discretizes the data.\n",
        "        Args:\n",
        "            data: numpy 2d or 1d array\n",
        "        Returns:\n",
        "            numpy array of same dimension, discretized.\n",
        "        \"\"\"\n",
        "        ret = data.copy()\n",
        "        for feature in self.lambdas:\n",
        "            if len(data.shape) == 1:\n",
        "                ret[feature] = int(self.lambdas[feature](ret[feature]))\n",
        "            else:\n",
        "                ret[:, feature] = self.lambdas[feature](\n",
        "                    ret[:, feature]).astype(int)\n",
        "        return ret\n",
        "\n",
        "    def get_undiscretize_values(self, feature, values):\n",
        "        mins = np.array(self.mins[feature])[values]\n",
        "        maxs = np.array(self.maxs[feature])[values]\n",
        "\n",
        "        means = np.array(self.means[feature])[values]\n",
        "        stds = np.array(self.stds[feature])[values]\n",
        "        minz = (mins - means) / stds\n",
        "        maxz = (maxs - means) / stds\n",
        "        min_max_unequal = (minz != maxz)\n",
        "\n",
        "        ret = minz\n",
        "        ret[np.where(min_max_unequal)] = scipy.stats.truncnorm.rvs(\n",
        "            minz[min_max_unequal],\n",
        "            maxz[min_max_unequal],\n",
        "            loc=means[min_max_unequal],\n",
        "            scale=stds[min_max_unequal],\n",
        "            random_state=self.random_state\n",
        "        )\n",
        "        return ret\n",
        "\n",
        "    def undiscretize(self, data):\n",
        "        ret = data.copy()\n",
        "        for feature in self.means:\n",
        "            if len(data.shape) == 1:\n",
        "                ret[feature] = self.get_undiscretize_values(\n",
        "                    feature, ret[feature].astype(int).reshape(-1, 1)\n",
        "                )\n",
        "            else:\n",
        "                ret[:, feature] = self.get_undiscretize_values(\n",
        "                    feature, ret[:, feature].astype(int)\n",
        "                )\n",
        "        return ret\n",
        "\n",
        "\n",
        "class StatsDiscretizer(BaseDiscretizer):\n",
        "    \"\"\"\n",
        "        Class to be used to supply the data stats info when discretize_continuous is true\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, categorical_features, feature_names, labels=None, random_state=None,\n",
        "                 data_stats=None):\n",
        "\n",
        "        BaseDiscretizer.__init__(self, data, categorical_features,\n",
        "                                 feature_names, labels=labels,\n",
        "                                 random_state=random_state,\n",
        "                                 data_stats=data_stats)\n",
        "\n",
        "    def bins(self, data, labels):\n",
        "        bins_from_stats = self.data_stats.get(\"bins\")\n",
        "        bins = []\n",
        "        if bins_from_stats is not None:\n",
        "            for feature in self.to_discretize:\n",
        "                bins_from_stats_feature = bins_from_stats.get(feature)\n",
        "                if bins_from_stats_feature is not None:\n",
        "                    qts = np.array(bins_from_stats_feature)\n",
        "                    bins.append(qts)\n",
        "        return bins\n",
        "\n",
        "\n",
        "class QuartileDiscretizer(BaseDiscretizer):\n",
        "    def __init__(self, data, categorical_features, feature_names, labels=None, random_state=None):\n",
        "\n",
        "        BaseDiscretizer.__init__(self, data, categorical_features,\n",
        "                                 feature_names, labels=labels,\n",
        "                                 random_state=random_state)\n",
        "\n",
        "    def bins(self, data, labels):\n",
        "        bins = []\n",
        "        for feature in self.to_discretize:\n",
        "            qts = np.array(np.percentile(data[:, feature], [25, 50, 75]))\n",
        "            bins.append(qts)\n",
        "        return bins\n",
        "\n",
        "\n",
        "class DecileDiscretizer(BaseDiscretizer):\n",
        "    def __init__(self, data, categorical_features, feature_names, labels=None, random_state=None):\n",
        "        BaseDiscretizer.__init__(self, data, categorical_features,\n",
        "                                 feature_names, labels=labels,\n",
        "                                 random_state=random_state)\n",
        "\n",
        "    def bins(self, data, labels):\n",
        "        bins = []\n",
        "        for feature in self.to_discretize:\n",
        "            qts = np.array(np.percentile(data[:, feature],\n",
        "                                         [10, 20, 30, 40, 50, 60, 70, 80, 90]))\n",
        "            bins.append(qts)\n",
        "        return bins\n",
        "\n",
        "\n",
        "class EntropyDiscretizer(BaseDiscretizer):\n",
        "    def __init__(self, data, categorical_features, feature_names, labels=None, random_state=None):\n",
        "        if(labels is None):\n",
        "            raise ValueError('Labels must be not None when using \\\n",
        "                             EntropyDiscretizer')\n",
        "        BaseDiscretizer.__init__(self, data, categorical_features,\n",
        "                                 feature_names, labels=labels,\n",
        "                                 random_state=random_state)\n",
        "\n",
        "    def bins(self, data, labels):\n",
        "        bins = []\n",
        "        for feature in self.to_discretize:\n",
        "            # Entropy splitting / at most 8 bins so max_depth=3\n",
        "            dt = sklearn.tree.DecisionTreeClassifier(criterion='entropy',\n",
        "                                                     max_depth=3,\n",
        "                                                     random_state=self.random_state)\n",
        "            x = np.reshape(data[:, feature], (-1, 1))\n",
        "            dt.fit(x, labels)\n",
        "            qts = dt.tree_.threshold[np.where(dt.tree_.children_left > -1)]\n",
        "\n",
        "            if qts.shape[0] == 0:\n",
        "                qts = np.array([np.median(data[:, feature])])\n",
        "            else:\n",
        "                qts = np.sort(qts)\n",
        "\n",
        "            bins.append(qts)\n",
        "\n",
        "        return bins"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7arAH9e0Z53F"
      },
      "source": [
        "def data_inverse(training_data,training_labels,sample_around_instance,num_samples,random_seed,data_row=None,\n",
        "                 kernel_width=None,kernel=None,categorical_features=None,\n",
        "                 categorical_names=None,feature_values=None,feature_names=None,feature_frequencies=None\n",
        "                       ): \n",
        "  discretize_continuous=True\n",
        "  training_data_stats=False\n",
        "  random_state = check_random_state(random_seed)\n",
        "  categorical_names = categorical_names or {}\n",
        "\n",
        "  if categorical_features is None:\n",
        "      categorical_features = []\n",
        "  if feature_names is None:\n",
        "      feature_names = [str(i) for i in range(training_data.shape[1])]\n",
        "\n",
        "  categorical_features = list(categorical_features)\n",
        "  feature_names = list(feature_names)\n",
        "  # if discretize_continuous and not sp.sparse.issparse(training_data):\n",
        "  #   discretizer = QuartileDiscretizer(\n",
        "  #               training_data, categorical_features,\n",
        "  #               feature_names, labels=training_labels,\n",
        "  #               random_state=random_state)\n",
        "  #   categorical_features = list(range(training_data.shape[1]))\n",
        "\n",
        "  #           # Get the discretized_training_data when the stats are not provided\n",
        "  #   discretized_training_data = discretizer.discretize(training_data)\n",
        "\n",
        "\n",
        "  if kernel_width is None:\n",
        "      kernel_width = np.sqrt(training_data.shape[1]) * .75\n",
        "  kernel_width = float(kernel_width)\n",
        "\n",
        "  if kernel is None:\n",
        "      def kernel(d, kernel_width):\n",
        "          return np.sqrt(np.exp(-(d ** 2) / kernel_width ** 2))\n",
        "\n",
        "  kernel_fn = partial(kernel, kernel_width=kernel_width)\n",
        "\n",
        "  # Though set has no role to play if training data stats are provided\n",
        "  scaler = sklearn.preprocessing.StandardScaler(with_mean=True)\n",
        "  scaler.fit(training_data)\n",
        "  feature_values = {}\n",
        "  feature_frequencies = {}\n",
        "  \n",
        "  for feature in categorical_features:\n",
        "      column = training_data.iloc[:, feature]\n",
        "\n",
        "      feature_count = collections.Counter(column)\n",
        "      values, frequencies = map(list, zip(*(sorted(feature_count.items()))))\n",
        "\n",
        "      feature_values[feature] = values\n",
        "      feature_frequencies[feature] = (np.array(frequencies) /\n",
        "                                            float(sum(frequencies)))\n",
        "      scaler.mean_[feature] = 0\n",
        "      scaler.scale_[feature] = 1\n",
        "  \"\"\"Generates a neighborhood around a prediction.\n",
        "  For numerical features, perturb them by sampling from a Normal(0,1) and\n",
        "  doing the inverse operation of mean-centering and scaling, according to\n",
        "  the means and stds in the training data. For categorical features,\n",
        "  perturb by sampling according to the training distribution, and making\n",
        "  a binary feature that is 1 when the value is the same as the instance\n",
        "  being explained.\n",
        "  Args:\n",
        "      data_row: 1d numpy array, corresponding to a row\n",
        "      num_samples: size of the neighborhood to learn the linear model\n",
        "      sampling_method: 'gaussian' or 'lhs'\n",
        "  Returns:\n",
        "      A tuple (data, inverse), where:\n",
        "          data: dense num_samples * K matrix, where categorical features\n",
        "          are encoded with either 0 (not equal to the corresponding value\n",
        "          in data_row) or 1. The first row is the original instance.\n",
        "          inverse: same as data, except the categorical features are not\n",
        "          binary, but categorical (as the original data)\n",
        "  \"\"\"\n",
        "  if sample_around_instance:\n",
        "    is_sparse = sp.sparse.issparse(data_row)\n",
        "    if is_sparse:\n",
        "        num_cols = data_row.shape[1]\n",
        "        data = sp.sparse.csr_matrix((num_samples, num_cols), dtype=data_row.dtype)\n",
        "    else:\n",
        "        num_cols = data_row.shape[0]\n",
        "        data = np.zeros((num_samples, num_cols))\n",
        "  else:\n",
        "    is_sparse=False\n",
        "    num_cols= train.shape[1]\n",
        "    data = np.zeros((num_samples, num_cols))\n",
        "\n",
        "  if categorical_features is None:\n",
        "    categorical_features = range(num_cols)\n",
        "  instance_sample = data_row\n",
        "  scale = scaler.scale_\n",
        "  mean = scaler.mean_\n",
        "  if is_sparse:\n",
        "      # Perturb only the non-zero values\n",
        "      non_zero_indexes = data_row.nonzero()[1]\n",
        "      num_cols = len(non_zero_indexes)\n",
        "      instance_sample = data_row[:, non_zero_indexes]\n",
        "      scale = scale[non_zero_indexes]\n",
        "      mean = mean[non_zero_indexes]\n",
        "      \n",
        "  data = random_state.normal(0, 1, num_samples * num_cols\n",
        "                                  ).reshape(num_samples, num_cols)\n",
        "  data = np.array(data)\n",
        "\n",
        "  if sample_around_instance:\n",
        "      data = data * scale + instance_sample\n",
        "  else:\n",
        "      data = data * scale + mean\n",
        "  inverse = data.copy()\n",
        "  for column in categorical_features:\n",
        "      values =feature_values[column]\n",
        "      freqs = feature_frequencies[column]\n",
        "      inverse_column = random_state.choice(values, size=num_samples,\n",
        "                                                replace=True, p=freqs)\n",
        "      # binary_column = (inverse_column == first_row[column]).astype(int)\n",
        "      binary_column=inverse_column\n",
        "      # binary_column[0] = 1\n",
        "      # inverse_column[0] = data[0, column]\n",
        "      data[:, column] = binary_column\n",
        "      inverse[:, column] = inverse_column\n",
        "  return data, inverse\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-dawmuY2hBj"
      },
      "source": [
        "def get_perturbed_points(dataset_name , seed):\n",
        "  if dataset_name is 'liver':\n",
        "    categorical_features=[0]\n",
        "    feature_values=[data[data.columns[0]].unique()]\n",
        "    new_data_points,inverse=data_inverse(train,labels_train,False,10000,random_seed=seed,feature_values=feature_values,categorical_features=categorical_features)\n",
        "    df_new_points=pd.DataFrame(new_data_points , columns=train.columns)\n",
        "    normalized_new=normalize_data(df_new_points,True)\n",
        "    return normalized_new,df_new_points,new_data_points\n",
        "  elif dataset_name is 'breast_cancer':\n",
        "    new_data_points,inverse=data_inverse(train,labels_train,False,10000,random_seed=seed)\n",
        "    df_new_points=pd.DataFrame(new_data_points)\n",
        "    normalized_new=normalize_data(df_new_points,False)\n",
        "    return normalized_new,df_new_points,new_data_points\n",
        "  elif dataset_name is 'hepatisis':\n",
        "    categorical_features=[0,1,2,3,4,5,6,7,8,9,10,11,12]\n",
        "    feature_values=[data[data.columns[i]].unique() for i in range(13)]\n",
        "    new_data_points,inverse=data_inverse(train,labels_train,False,10000,random_seed=seed,feature_values=feature_values,categorical_features=categorical_features)\n",
        "    df_new_points=pd.DataFrame(new_data_points , columns=train.columns)\n",
        "    normalized_new=normalize_data(df_new_points,True)\n",
        "    return normalized_new,df_new_points,new_data_points\n",
        "  elif dataset_name is 'adults':\n",
        "    categorical_features=range(6,105)\n",
        "    feature_values=[data[data.columns[i]].unique() for i in range(6,105)]\n",
        "    new_data_points,inverse=data_inverse(train,labels_train,False,5000,random_seed=seed,feature_values=feature_values,categorical_features=categorical_features)\n",
        "    df_new_points=pd.DataFrame(new_data_points , columns=train.columns)\n",
        "    normalized_new=normalize_data(df_new_points,True)\n",
        "    return normalized_new,df_new_points,new_data_points\n",
        "  else:\n",
        "    print(\"dataset_name is not correct\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5UXGLYH42SJ"
      },
      "source": [
        "normalized_new,df_new_points,new_data_points=get_perturbed_points('breast_cancer',5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfasvbv95n-H"
      },
      "source": [
        "normalized_new,df_new_points,new_data_points=get_perturbed_points('liver',5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2H-QKRz_5pos"
      },
      "source": [
        "normalized_new,df_new_points,new_data_points=get_perturbed_points('hepatisis',5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzIIBSXt5rWf"
      },
      "source": [
        "normalized_new,df_new_points,new_data_points=get_perturbed_points('adults',5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1t_j30-n5Sw7"
      },
      "source": [
        "normalized_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YqtFshljp3v"
      },
      "source": [
        "def get_AE_output(input_data):\n",
        "  new_df=pd.DataFrame()\n",
        "  for i in ['1','2','3']:\n",
        "    columns_names = ['Hidden_'+str(i)+'_'+str(l) for l in range(0, len_input_columns*10)]\n",
        "    for l in columns_names:\n",
        "       new_df[l] = 0 # create columns (maybe it's not optimized)\n",
        "    intermediate_layer_model = Model(inputs=model_dae.input, outputs=model_dae.get_layer('Hidden' + i).output)\n",
        "    new_df[columns_names] = intermediate_layer_model.predict(input_data)\n",
        "  return new_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7n8T0p6RXbj"
      },
      "source": [
        "# Train black-box model (here a feed forward model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARSBIU9Xtlns"
      },
      "source": [
        "def main_model_fit(train1, labels_train1,val,labels_val, K,num_units1,num_units2=None):\n",
        "  early_stopping=tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', min_delta=0, patience=1, verbose=0,\n",
        "    mode='auto', baseline=None, restore_best_weights=False)\n",
        "  if num_units2 is not None: \n",
        "     model=tf.keras.models.Sequential([\n",
        "                                    tf.keras.layers.Dense(units=num_units1),\n",
        "                                    tf.keras.layers.Dense(units=num_units2)\n",
        "                                   ,tf.keras.layers.Dense(1,activation='sigmoid')]) \n",
        "  else:\n",
        "     model=tf.keras.models.Sequential([\n",
        "                                    tf.keras.layers.Dense(units=num_units1)\n",
        "                                   ,tf.keras.layers.Dense(1,activation='sigmoid')]) \n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "                loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                metrics=['accuracy'])\n",
        "  history= model.fit(train1, labels_train1,validation_data=(val,labels_val) ,epochs=100,verbose=0  ,callbacks=[early_stopping] )\n",
        "  return history,model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yauFP2AI6Ev1"
      },
      "source": [
        "def main_model_fit_final(train1, labels_train1,K,num_units1,num_units2=None):\n",
        "  if num_units2 is not None: \n",
        "     model=tf.keras.models.Sequential([\n",
        "                                    tf.keras.layers.Dense(units=num_units1),\n",
        "                                    tf.keras.layers.Dense(units=num_units2)\n",
        "                                   ,tf.keras.layers.Dense(1,activation='sigmoid')]) \n",
        "  else:\n",
        "     model=tf.keras.models.Sequential([\n",
        "                                    tf.keras.layers.Dense(units=num_units1)\n",
        "                                   ,tf.keras.layers.Dense(1,activation='sigmoid')]) \n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "                loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                metrics=['accuracy'])\n",
        "  history= model.fit(train1, labels_train1,epochs=100,verbose=1  )\n",
        "  return history,model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGBduDkp7kBH"
      },
      "source": [
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1drlGYus5Ko-"
      },
      "source": [
        "tf.random.set_seed(0)\n",
        "history,model=main_model_fit_final(normalized_train,labels_train,20,5)\n",
        "predict_labels=model.predict(normalized_test)\n",
        "labels=[0 if x<=0.5 else 1 for x in predict_labels]\n",
        "acc_test=metrics.accuracy_score(labels_test,labels)\n",
        "print(acc_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pafqOQVv7HbP"
      },
      "source": [
        "num_units=[5,10,15,20,25,30,35]\n",
        "tf.random.set_seed(0)\n",
        "kf = KFold(n_splits=10)\n",
        "average_accuracy=0\n",
        "df_labels=pd.DataFrame(labels)\n",
        "# df_data=pd.DataFrame(data)\n",
        "for train_index, test_index in kf.split(normalized_data):\n",
        "  X_train, X_test = normalized_data.iloc[train_index], normalized_data.iloc[test_index]\n",
        "  y_train, y_test = df_labels.iloc[train_index], df_labels.iloc[test_index]\n",
        "  (train1,val,labels_train1,labels_val) = model_selection.train_test_split(X_train, y_train, train_size=0.90)\n",
        "  min_loss=1000\n",
        "  for unit1 in num_units:\n",
        "    print(unit1)\n",
        "    for unit2 in num_units:\n",
        "      history,model=main_model_fit(train1,labels_train1,val,labels_val,unit1,unit2)\n",
        "      loss=history.history['val_loss'][len(history.history['val_loss'])-1]\n",
        "      if loss<min_loss:\n",
        "        min_loss=loss\n",
        "        best_model=model\n",
        "        best_accuracy=history.history['val_accuracy'][len(history.history['val_accuracy'])-1]\n",
        "        best_num_unit1=unit1\n",
        "        best_num_unit2=unit2\n",
        "    # print(history.history['val_accuracy'])\n",
        "  print(best_accuracy)\n",
        "  print(best_num_unit1)\n",
        "  print(best_num_unit2)\n",
        "  predict_labels=best_model.predict(X_test)\n",
        "  labels=[0 if x<=0.5 else 1 for x in predict_labels]\n",
        "  acc_test=metrics.accuracy_score(y_test,labels)\n",
        "  average_accuracy+=acc_test\n",
        "  print(acc_test)\n",
        "average_accuracy=average_accuracy/10.0\n",
        "print(average_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySHzXtRmkthN",
        "outputId": "4084c90f-5bd6-4453-e34b-c662469dca5b"
      },
      "source": [
        "predict_labels=model.predict(normalized_test)\n",
        "labels=[0 if x<=0.5 else 1 for x in predict_labels]\n",
        "metrics.accuracy_score(labels_test,labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8533114955471389"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSFfO-R2AZUi"
      },
      "source": [
        "# model=best_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqfiMgI8RonK"
      },
      "source": [
        "# ALIME and tree_ALIME"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJsmfitA5v32"
      },
      "source": [
        "def ALIME(x, n , model ,new_data_points,normalized_new):\n",
        "  embedded_new_data_points=get_AE_output(normalized_new)\n",
        "  embedded_x=get_AE_output(x)\n",
        "  emb_new_array=np.array(embedded_new_data_points)\n",
        "  emb_x_array=np.array(embedded_x)\n",
        "  d=np.linalg.norm(emb_new_array-emb_x_array , axis=1)\n",
        "  dmin=heapq.nsmallest(n, d)[-1]\n",
        "  local_dataset_norm=[]\n",
        "  local_dataset=[]\n",
        "  weights=np.zeros(n)\n",
        "  count=0\n",
        "  for i in range(len(d)):\n",
        "     if d[i]<=dmin:\n",
        "       local_dataset.append(new_data_points[i])\n",
        "       local_dataset_norm.append(normalized_new.iloc[i])\n",
        "       weights[count]=np.exp(-d[i])\n",
        "       count+=1\n",
        "  \n",
        "  df_local_dataset=pd.DataFrame(local_dataset_norm,columns=dae_data.columns)\n",
        "  predict_labels=model.predict(df_local_dataset)\n",
        "  labels=[0 if x<=0.5 else 1 for x in predict_labels]\n",
        "  sum=np.sum(labels)\n",
        "  l_model=None\n",
        "  predict=None\n",
        "  if sum==0 or sum==len(labels):\n",
        "    predict=sum/len(labels)\n",
        "  else:\n",
        "    l_model=sklearn.linear_model.LogisticRegression(max_iter=150,solver='liblinear', random_state=0)\n",
        "    l_model.fit(local_dataset_norm,labels,sample_weight=weights )\n",
        "  return l_model,predict\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zD9aAw80Vug"
      },
      "source": [
        "def ALIME_linear_and_tree(x, n , model ,new_data_points,normalized_new):\n",
        "  embedded_new_data_points=get_AE_output(normalized_new)\n",
        "  embedded_x=get_AE_output(x)\n",
        "  emb_new_array=np.array(embedded_new_data_points)\n",
        "  emb_x_array=np.array(embedded_x)\n",
        "  d=np.linalg.norm(emb_new_array-emb_x_array , axis=1)\n",
        "  dmin=heapq.nsmallest(n, d)[-1]\n",
        "  local_dataset_norm=[]\n",
        "  local_dataset=[]\n",
        "  weights=np.zeros(n)\n",
        "  count=0\n",
        "  for i in range(len(d)):\n",
        "     if d[i]<=dmin:\n",
        "       local_dataset.append(new_data_points[i])\n",
        "       local_dataset_norm.append(normalized_new.iloc[i])\n",
        "       weights[count]=np.exp(-d[i])\n",
        "       count+=1\n",
        "  \n",
        "  df_local_dataset=pd.DataFrame(local_dataset_norm,columns=dae_data.columns)\n",
        "  predict_labels=model.predict(df_local_dataset)\n",
        "  labels=[0 if x<=0.5 else 1 for x in predict_labels]\n",
        "  sum=np.sum(labels)\n",
        "  l_model=None\n",
        "  tree_model=None\n",
        "  predict=None\n",
        "  if sum==0 or sum==len(labels):\n",
        "    predict=sum/len(labels)\n",
        "  else:\n",
        "    l_model=sklearn.linear_model.LogisticRegression(max_iter=150,solver='liblinear', random_state=0)\n",
        "    l_model.fit(local_dataset_norm,labels,sample_weight=weights )\n",
        "\n",
        "    tree_model=sklearn.tree.DecisionTreeClassifier(random_state=0,max_depth=5)\n",
        "    tree_model.fit(local_dataset,labels,sample_weight=weights)\n",
        "\n",
        "  return l_model,tree_model,predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nXThz7hAOxY"
      },
      "source": [
        "def test_on_a_data_ALIME(num_features, num_data):\n",
        "  dae_test = normalized_test\n",
        "  dat_norm=dae_test.iloc[num_data]\n",
        "  series_obj = pd.Series(dat_norm)\n",
        "  arr = series_obj.values\n",
        "  x = arr.reshape((1, num_features))\n",
        "  l_model,tree_model,predict= ALIME_linear_and_tree(x,300, model ,new_data_points,normalized_new)\n",
        "  return x,l_model,tree_model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bc2XQpYsoDNj"
      },
      "source": [
        "x,l_model,tree_model=test_on_a_data_ALIME(train.shape[1],10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZF6Fpw1Abvgw"
      },
      "source": [
        "l_model.coef_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9m8KSgyb40F"
      },
      "source": [
        "import graphviz\n",
        "import os\n",
        "def visualize_ALIME(dataset_name):\n",
        "  i=0\n",
        "  count=[]\n",
        "  cols=['num']\n",
        "  cols+=list(df_new_points.columns)\n",
        "  df_data=pd.DataFrame(columns=cols)\n",
        "  while len(count)<20:\n",
        "    num_test=np.random.randint(0,len(normalized_test))\n",
        "    while num_test in count:\n",
        "      num_test=np.random.randint(0,len(normalized_test))\n",
        "    print(num_test)\n",
        "    print(test.iloc[num_test])\n",
        "    print('-----------')\n",
        "    dat=[num_test]\n",
        "    dat+=list(test.iloc[num_test])\n",
        "    df_data.loc[i]=dat\n",
        "    i+=1\n",
        "    print(df_data)\n",
        "    count.append(num_test)\n",
        "    x,l_model,tree_model=test_on_a_data_ALIME(train.shape[1],num_test)\n",
        "    fig = plt.figure(figsize =(10,10))\n",
        "    coefs=[]\n",
        "    feature_names=[]\n",
        "    select_df = pd.DataFrame(columns=['coefs','feature_name'])\n",
        "    if l_model !=None and tree_model!=None:\n",
        "      coefs_row= l_model.coef_.reshape(-1)\n",
        "      df = pd.DataFrame({'coefs':coefs_row,\n",
        "                        'feature_name':train.columns})\n",
        "      df = df.sort_values(by='coefs')\n",
        "      select_df = df.head(5).append(df.tail(5))\n",
        "    if l_model !=None :\n",
        "      bar=plt.barh(select_df.feature_name, select_df.coefs)\n",
        "      plt.xlabel('weights')\n",
        "      plt.ylabel('features')\n",
        "      fig.savefig('{}/linear_model/{}.png'.format(dataset_name,num_test))\n",
        "\n",
        "    if tree_model!=None:\n",
        "      print(class_names)\n",
        "      dot_data=sklearn.tree.export_graphviz(tree_model, out_file=None, \n",
        "                                    feature_names=train.columns,  \n",
        "                                    class_names=class_names,\n",
        "                                    filled=True,impurity=False)\n",
        "      graph = graphviz.Source(dot_data, format=\"png\") \n",
        "      graph.render('{}/tree_model/{}'.format(dataset_name,num_test))\n",
        "  df_data.to_csv('adults_test.csv')\n",
        "  return df_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tu_VtnttC8RZ"
      },
      "source": [
        "visualize_ALIME('adults')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF0d1NcVh7V0"
      },
      "source": [
        "!zip -r /content/adults.zip /content/adults"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3Ydw81FaxVy"
      },
      "source": [
        "visualize_ALIME('breastCancer')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfEIo_mbe2cP"
      },
      "source": [
        "!zip -r /content/breast_cancer.zip /content/breastCancer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQchWqdUhjyi"
      },
      "source": [
        "visualize_ALIME('hepatisis')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUqVjragG32H"
      },
      "source": [
        "def ALIME_decision_Tree(x , n ,  model,new_data_points,normalized_new):\n",
        "  embedded_new_data_points=get_AE_output(normalized_new)\n",
        "  embedded_x=get_AE_output(x)\n",
        "  emb_new_array=np.array(embedded_new_data_points)\n",
        "  emb_x_array=np.array(embedded_x)\n",
        "  d=np.linalg.norm(emb_new_array-emb_x_array , axis=1)\n",
        "  dmin=heapq.nsmallest(n, d)[-1]\n",
        "  local_dataset_norm=[]\n",
        "  local_dataset=[]\n",
        "  weights=np.zeros(n)\n",
        "  count=0\n",
        "  for i in range(len(d)):\n",
        "     if d[i]<=dmin:\n",
        "       local_dataset.append(new_data_points[i])\n",
        "       local_dataset_norm.append(normalized_new.iloc[i])\n",
        "       weights[count]=np.exp(-d[i])\n",
        "       count+=1\n",
        "  \n",
        "  df_local_dataset=pd.DataFrame(local_dataset_norm,columns=dae_data.columns)\n",
        "  predict_labels=model.predict(df_local_dataset)\n",
        "  labels=[0 if x<=0.5 else 1 for x in predict_labels]\n",
        "  tree_model=sklearn.tree.DecisionTreeClassifier(random_state=0)\n",
        "  tree_model.fit(local_dataset,labels,sample_weight=weights)\n",
        "  return tree_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9C2k4E36A5YJ"
      },
      "source": [
        "def test_on_a_data_ALIME_decision_Tree(num_features, num_data):\n",
        "  dae_test = normalized_test\n",
        "  dat_norm=dae_test.iloc[num_data]\n",
        "  series_obj = pd.Series(dat_norm)\n",
        "  arr = series_obj.values\n",
        "  x = arr.reshape((1, num_features))\n",
        "  tree_model=ALIME_decision_Tree(x,100,model,new_data_points,normalized_new)\n",
        "  return x,tree_model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cnauKCv1s9a"
      },
      "source": [
        "x,tree_model=test_on_a_data_ALIME_decision_Tree(train.shape[1],10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZzscSN415Hw"
      },
      "source": [
        "tree_model.feature_importances_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaRwxcmlST8S"
      },
      "source": [
        "data.feature_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_yOdYycHM1Y"
      },
      "source": [
        "text_representation = tree.export_text(tree_model , feature_names=list(data.feature_names))\n",
        "print(text_representation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wChyDIiCTwQv"
      },
      "source": [
        "fig = plt.figure(figsize=(25,20))\n",
        "_ = tree.plot_tree(tree_model, \n",
        "                   feature_names=data.feature_names,  \n",
        "                   class_names=data.target_names,\n",
        "                   filled=True ,impurity=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bok9uYLyRwsk"
      },
      "source": [
        "# Evaluate Fidelity and Stability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iw4gNl7Jhoth"
      },
      "source": [
        "def evaluate_fidelity(model,K,dataset_name):\n",
        "  dae_test = normalized_test.sample(n=1000)\n",
        "  labels= model.predict(dae_test)\n",
        "  truth_labels=[0 if x<=0.5 else 1 for x in labels]\n",
        "  f=open(\"drive/MyDrive/accuracies_{}.txt\".format(dataset_name),'w')\n",
        "  truth_labels_all=[]\n",
        "  for num_data in range(100,1500,100):\n",
        "    predict_labels1=[]\n",
        "    predict_labels2=[]\n",
        "    print(num_data)\n",
        "    for i in range(len(dae_test)):\n",
        "      dat= dae_test.iloc[i]\n",
        "      series_obj = pd.Series(dat)\n",
        "      arr = series_obj.values\n",
        "      x = arr.reshape((1, K))\n",
        "      l_model,tree_model,predict=ALIME_linear_and_tree(x,num_data,model,new_data_points,normalized_new)\n",
        "      if predict==None:\n",
        "        predict_labels1.append(l_model.predict(x)[0])\n",
        "        predict_labels2.append(tree_model.predict(x)[0])\n",
        "      else:\n",
        "        predict_labels1.append(predict)\n",
        "        predict_labels2.append(predict)\n",
        "      truth_labels_all.append(truth_labels[i])\n",
        "      if i%5==0:\n",
        "        print(i)\n",
        "    f.write(str(num_data))\n",
        "    f.write(\"\\n\")\n",
        "    accuracy1=metrics.accuracy_score(truth_labels_all,predict_labels1)\n",
        "    f.write(str(accuracy1))\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "    accuracy2=metrics.accuracy_score(truth_labels_all,predict_labels2)\n",
        "    f.write(str(accuracy2))\n",
        "  \n",
        "    f.write(\"\\n\")\n",
        "    f.write('----------')\n",
        "    f.write(\"\\n\")\n",
        "    f.flush()\n",
        "\n",
        "    print(accuracy1)\n",
        "    print(accuracy2)\n",
        "\n",
        "  f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwBMQmcRawTo"
      },
      "source": [
        "evaluate_fidelity(model,train.shape[1],'adults')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0YX1ZxCBk8u"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_fidelity(dataset_name):\n",
        "  f=open(\"accuracies_{}.txt\".format(dataset_name),'r')\n",
        "  lines=f.readlines()\n",
        "  nums=[]\n",
        "  accs_alime=[]\n",
        "  accs_tree=[]\n",
        "  for i in range(0,len(lines),4):\n",
        "    num_data=lines[i]\n",
        "    acc_alime=lines[i+1]\n",
        "    acc_tree=lines[i+2]\n",
        "    nums.append(int(num_data))\n",
        "    accs_alime.append(float(acc_alime))\n",
        "    accs_tree.append(float(acc_tree))\n",
        "  \n",
        "  plt.title(dataset_name)\n",
        "  plt.xlabel(\"num_data\")\n",
        "  plt.ylabel(\"accuracy\")\n",
        "  plt.plot(nums,accs_alime, label='linear')\n",
        "  plt.plot(nums,accs_tree , label='decision_tree')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujc3VBkJD6d-"
      },
      "source": [
        "plot_fidelity('breast_cancer')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPrynudDFqll"
      },
      "source": [
        "plot_fidelity('hepatisis')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUhhRA5GE_Cr"
      },
      "source": [
        "def get_pos_And_neg_ind(array):\n",
        "  pos=[]\n",
        "  neg=[]\n",
        "  count=0\n",
        "  shape=array.shape\n",
        "  for i in range(shape[0]):\n",
        "    for j in range(shape[1]):\n",
        "      if array[i][j]>=0.01:\n",
        "        pos.append(count)\n",
        "      elif array[i][j]<=-0.01:\n",
        "        neg.append(count)\n",
        "      count+=1\n",
        "  return pos,neg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6P3QJHaAvpR"
      },
      "source": [
        "def evaluate_stability(model,K,dataset_name):\n",
        "  dae_test = normalized_test\n",
        "  dat_norm=dae_test.iloc[np.random.randint(0,len(dae_test))]\n",
        "  series_obj = pd.Series(dat_norm)\n",
        "  arr = series_obj.values\n",
        "  x_norm = arr.reshape((1, K))\n",
        "  f= open('drive/MyDrive/stability_{}.txt'.format(dataset_name),'w')\n",
        "  for num_data in range(100,1600,100):\n",
        "    print(num_data)\n",
        "    f.write(str(num_data))\n",
        "    f.write('\\n')\n",
        "    tree_features=[]\n",
        "    l_features_pos=[]\n",
        "    l_features_neg=[]\n",
        "    for seed in range(20):\n",
        "      if seed%5==0:\n",
        "        print(seed)\n",
        "      normalized_new,df_new_points,new_data_points=get_perturbed_points(dataset_name,seed)\n",
        "      l_model,tree_model,predict=ALIME_linear_and_tree(x_norm,num_data,model,new_data_points,normalized_new)\n",
        "      pos,neg=get_pos_And_neg_ind(l_model.coef_)\n",
        "      l_features_pos.append(pos)\n",
        "      l_features_neg.append(neg)\n",
        "      important_features_tree=np.nonzero(tree_model.feature_importances_)\n",
        "      tree_features.append(important_features_tree)\n",
        "  #stability of ALIME linear\n",
        "    res_pos=0\n",
        "    count_pos=0\n",
        "    sum_len_pos=0\n",
        "    for i in range(len(l_features_pos)):\n",
        "      sum_len_pos+=len(l_features_pos[i])\n",
        "      for j in range(len(l_features_pos)):\n",
        "        if i != j:\n",
        "          set1=set(l_features_pos[i])\n",
        "          set2=set(l_features_pos[j])\n",
        "          union1=set1.union(set2)\n",
        "          intersection1=set1.intersection(set2)\n",
        "          if len(union1)>0:\n",
        "            res_pos+=len(intersection1)/len(union1)\n",
        "            count_pos+=1\n",
        "    res_neg=0\n",
        "    count_neg=0\n",
        "    sum_len_neg=0\n",
        "    for i in range(len(l_features_neg)):\n",
        "      sum_len_neg+=len(l_features_neg[i])\n",
        "      for j in range(len(l_features_neg)):\n",
        "        if i != j:\n",
        "          set1=set(l_features_neg[i])\n",
        "          set2=set(l_features_neg[j])\n",
        "          union1=set1.union(set2)\n",
        "          intersection1=set1.intersection(set2)\n",
        "          if len(union1)>0:\n",
        "            res_neg+=len(intersection1)/len(union1)\n",
        "            count_neg+=1\n",
        "    ave_len_all=(sum_len_neg+sum_len_pos) /(len(l_features_pos)+len(l_features_neg))\n",
        "    stab_pos=0\n",
        "    stab_neg=0\n",
        "    if count_pos>0:\n",
        "      stab_pos=res_pos/count_pos\n",
        "    if count_neg>0:\n",
        "      stab_neg=res_neg/count_neg\n",
        "    \n",
        "    stab_all=(stab_pos+stab_neg)/2.0\n",
        "    f.write(str(ave_len_all))\n",
        "    f.write('\\n')\n",
        "    f.write(str(stab_all))\n",
        "    f.write('\\n')\n",
        "    print('average_length ={}'.format(ave_len_all))\n",
        "    print('ALIME linear stability = {}'.format(stab_all))\n",
        "\n",
        "    #stability of ALIME tree\n",
        "    res=0\n",
        "    count=0\n",
        "    sum_len=0\n",
        "    for i in range(len(tree_features)):\n",
        "      sum_len+=len(tree_features[i][0])\n",
        "      for j in range(len(tree_features)):\n",
        "        if i is not j:\n",
        "          set1=set(tree_features[i][0])\n",
        "          set2=set(tree_features[j][0])\n",
        "          union1=set1.union(set2)\n",
        "          intersection1=set1.intersection(set2)\n",
        "          if len(union1)>0:\n",
        "             res+=len(intersection1)/len(union1)\n",
        "             count+=1\n",
        "    f.write(str(sum_len/len(tree_features)))\n",
        "    f.write('\\n')\n",
        "    f.write(str(res/count))\n",
        "    f.write('\\n')\n",
        "    f.write('--------------')\n",
        "    f.write('\\n')\n",
        "    f.flush()\n",
        "    print('average_length ={}'.format(sum_len/len(tree_features)))\n",
        "    print('ALIME tree stability = {}'.format(res/count))\n",
        "  f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwrRGjnYCuUn"
      },
      "source": [
        "evaluate_stability(model,train.shape[1],'adults')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}